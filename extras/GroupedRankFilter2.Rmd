---
title: "Grouped Rank Filter"
output: github_document
---

This is an experiment comparing the performance of a number of data processing systems available in [<code>R</code>](https://www.r-project.org).  Our example problem is finding the top ranking item per group (group defined by three columns: <code>col_a</code>,
<code>col_b</code>, <code>col_b</code>; and order defined by a single column <code>col_x</code>).  This is a common often needed task.



```{r pythonsetup}
# https://cran.r-project.org/web/packages/reticulate/vignettes/r_markdown.html
library("reticulate")
use_python("/home/ruser/miniconda3/bin/python3")
# use_python("/Users/johnmount/anaconda3/bin/python3")
pandas_handle <- reticulate::import("pandas") # don't use as https://github.com/rstudio/reticulate/issues/319

pandas_fn <- py_run_string("
def py_fn(df):
   ord = df.sort_values(by = ['col_a', 'col_b', 'col_c', 'col_x'], ascending = [True, True, True, True])
   ord['rank_col'] = ord.groupby(['col_a', 'col_b', 'col_c']).cumcount()
   return ord[ord.rank_col == 0].ix[:, ['col_a', 'col_b', 'col_c', 'col_x']]
")
do_pandas <- function(d) {
  res <- pandas_fn$py_fn(pandas_handle$DataFrame(d))
  rownames(res) <- NULL
  return(res)
}
```


```{r setup}
library("rqdatatable")
library("microbenchmark")
library("ggplot2")
library("WVPlots")
library("cdata")
library("dplyr")
library("dtplyr")
library("data.table")

set.seed(32523)

mk_data <- function(nrow) {
  alphabet <- paste("sym", seq_len(max(2, floor(nrow^(1/3)))), sep = "_")
  data.frame(col_a = sample(alphabet, nrow, replace=TRUE),
             col_b = sample(alphabet, nrow, replace=TRUE),
             col_c = sample(alphabet, nrow, replace=TRUE),
             col_x = runif(nrow),
             stringsAsFactors = FALSE)
}
```


```{r def}
# adapted from help(microbenchmark)
my_check <- function(values) {
  values <- lapply(values,
                   function(vi) {
                     vi <- as.data.frame(vi)
                     rownames(vi) <- NULL
                     data.frame(vi) # strip attributes
                   })
  isTRUE(all(sapply(values[-1], function(x) identical(values[[1]], x))))
}
```



```{r baser}
base_r <- function(df) {
  rownames(df) <- NULL
  df <- df[order(df$col_a, df$col_b, df$col_c, df$col_x, method = 'radix'), , 
           drop = FALSE]
  rownames(df) <- NULL
  n <- length(df$col_a)
  first <- c(TRUE,
             (df$col_a[-1] != df$col_a[-n]) | 
               (df$col_b[-1] != df$col_b[-n]) | 
               (df$col_c[-1] != df$col_c[-n]))
  df <- df[first, , drop = FALSE]
  rownames(df) <- NULL
  df
}
```



```{r time}
pow <- 8
rds_name <- "GroupedRankFilter2_runs.RDS"
if(!file.exists(rds_name)) {
  szs <- expand.grid(a = c(1,2,5), b = 10^{0:pow}) 
  szs <- sort(unique(szs$a * szs$b))
  szs <- szs[szs<=10^pow]
  runs <- lapply(
    rev(szs),
    function(sz) {
      gc()
      d <- mk_data(sz)
      ti <- microbenchmark(
        base_r = {
          base_r(d)
        },
        data.table = { 
          # https://stackoverflow.com/questions/16325641/how-to-extract-the-first-n-rows-per-group
          d %.>% 
            as.data.table(.) %.>% 
            setorder(., col_a, col_b, col_c, col_x) %.>%
            .[, .SD[1], by=list(col_a, col_b, col_c)] 
        },
        rqdatatable = { 
          ops <- local_td(d) %.>%
            pick_top_k(., 
                       k = 1L,
                       orderby = "col_x",
                       partitionby = c("col_a", "col_b", "col_c"),
                       keep_order_column = FALSE) %.>%
            orderby(., c("col_a", "col_b", "col_c", "col_x"))
          d %.>% ops
        },
        dplyr = {
          d %>% 
            group_by(col_a, col_b, col_c) %>% 
            arrange(col_x) %>% 
            filter(row_number() == 1) %>%
            ungroup() %>%
            arrange(col_a, col_b, col_c, col_x)
        },
        dplyr_b = {
          d %>% 
            arrange(col_x) %>% 
            group_by(col_a, col_b, col_c) %>% 
            mutate(rn = row_number()) %>%
            ungroup() %>%
            filter(rn == 1) %>%
            select(col_a, col_b, col_c, col_x) %>%
            arrange(col_a, col_b, col_c, col_x)
        },
        pandas_reticulate = {
          do_pandas(d)
        },
        times = 3L,
        check = my_check)
      ti <- as.data.frame(ti)
      ti$rows <- sz
      ti
    })
  saveRDS(runs, rds_name)
} else {
  runs <- readRDS(rds_name)
}
```

First let's compare three methods on the same grouped ranking problem.

  * [<code>dplyr</code>](https://CRAN.R-project.org/package=dplyr)
  * Base <code>R</code> (term defined as <code>R</code> plus just core packages, earlier results [here](http://www.win-vector.com/blog/2018/01/base-r-can-be-fast/)).
  * The seemingly silly idea of using [<code>reticulate</code>](https://CRAN.R-project.org/package=reticulate) to ship the data to <code>Python</code>, and then using [<code>Pandas</code>](https://pandas.pydata.org) to do the work, and finally bring the result back to <code>R</code>.

```{r present1, fig.retina=2, fig.width=12, fig.height=8}
timings <- do.call(rbind, runs)
timings$seconds <- timings$time/1e+9
timings$method <- factor(timings$expr)
timings$method <- reorder(timings$method, -timings$seconds)
method_map <- c(dplyr = "dplyr", 
                dplyr_b = "dplyr",
                pandas_reticulate = "base-R or R/python roundtrip",
                data.table = "data.table",
                rqdatatable = "data.table",   
                base_r  = "base-R or R/python roundtrip")
color_map <- c(
   dplyr = "#e7298a",
   dplyr_b = "#d95f02",
   pandas_reticulate = "#e6ab02",
   data.table = "#66a61e",
   rqdatatable = "#1b9e77",
   base_r = "#7570b3")
timings$method_family <- method_map[as.character(timings$method)]
timings$method_family <- reorder(timings$method_family, -timings$seconds)
rowset <- sort(unique(timings$rows))
smooths <- lapply(
  unique(as.character(timings$method)),
  function(mi) {
    ti <- timings[timings$method == mi, , drop = FALSE]
    ti$rows <- log(ti$rows)
    si <- loess(log(seconds) ~ rows, data = ti)
    pi <- data.frame(
      method = mi,
      rows = log(rowset),
      stringsAsFactors = FALSE)
    pi$seconds <- exp(predict(si, newdata = pi))
    pi$rows <- rowset
    pi
  })
smooths <- do.call(rbind, smooths)
smooths$method <- factor(smooths$method, levels = levels(timings$method))
```


```{r present2, fig.retina=2, fig.width=12, fig.height=8}
ggplot(data = timings[timings$method %in% qc(dplyr, base_r, pandas_reticulate),], 
       aes(x = rows, y = seconds)) +
  geom_point(aes(color = method)) + 
  geom_smooth(aes(color = method),
              se = FALSE) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_manual(values = color_map[qc(dplyr, base_r, "pandas_reticulate")]) +
  ggtitle("grouped ranked selection task time by rows and method",
          subtitle = "log-log trend shown; comparing dplyr, base-R, Python round-trip") 
```


Notice, contrary to many claims, <code>dplyr</code> is slower (higher up on the graph) than base <code>R</code> for all problem scales tested (1 row through 100,000,000 rows).  Height differences on a <code>log-y</code> scaled graph such as this represent ratios of run-times and we can see the ratio of <code>dplyr</code> to base-<code>R</code> runtime is routinely around a multiplicative factor of 50.

Also notice by the time we get the problem size up to 5,000 rows even sending the data to <code>Python</code> and back for <code>Pandas</code> processing is faster than <code>dplyr</code>.

Note: in this article "<code>pandas</code> timing" means the time it would take an <code>R</code> process to use <code>Pandas</code> for data manipulation.  This includes the extra overhead of moving the data from <code>R</code> to <code>Python</code>/<code>Pandas</code> and back. This is always going to be slower than <code>Pandas</code> itself as it includes extra overhead.  The point is we are not running a test designed to compare (or capable of comparing) <code>Pandas</code> to [<code>data.table</code>](https://CRAN.R-project.org/package=data.table) (you can already find such a study [here](https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping)), but instead performing a one-sided test of how <code>dplyr</code> compares to <code>Pandas</code> *plus* extra transport costs (so it is interesting if <code>Pandas</code> is faster, or even competitive, in this set-up, but not informative if <code>Pandas</code> plus extra costs is slower).

All runs were performed on an Amazon EC2 `r4.8xlarge` (244 GiB RAM) 64-bit Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-ba602bc2. We used R 3.4.4, with all packages current as of 8-20-2018 (the date of the experiment).

We are not testing [<code>dtplyr</code>](https://CRAN.R-project.org/package=dtplyr) for the simple reason it does not work with the <code>dplyr</code> pipeline as written.

```{r dtplyr, error=TRUE}
ds <- mk_data(3)

ds %>%  
  group_by(col_a, col_b, col_c) %>% 
  arrange(col_x) %>% 
  filter(row_number() == 1) %>%
  ungroup() %>%
  arrange(col_a, col_b, col_c, col_x)

ds %>%  
  as.data.table() %>%
  group_by(col_a, col_b, col_c) %>% 
  arrange(col_x) %>% 
  filter(row_number() == 1) %>%
  ungroup() %>%
  arrange(col_a, col_b, col_c, col_x)
```

For our example we used what I consider the natural <code>dplyr</code> solution to the problem.  The code looks like the following.

```{r dplyrc1, eval=FALSE}
d %>% 
  group_by(col_a, col_b, col_c) %>% 
  arrange(col_x) %>% 
  filter(row_number() == 1) %>%
  ungroup() %>%
  arrange(col_a, col_b, col_c, col_x)
```

<code>dplyr</code> has [known (unfixed) issues with filtering in the presence of grouping](https://github.com/tidyverse/dplyr/issues/3294).  Let's try to work around that with the following code (pivoting as many operations out of the grouped data section of the pipeline). 

```{r dplyrc12, eval=FALSE}
d %>% 
  arrange(col_x) %>% 
  group_by(col_a, col_b, col_c) %>% 
  mutate(rn = row_number()) %>%
  ungroup() %>%
  filter(rn == 1) %>%
  select(col_a, col_b, col_c, col_x) %>%
  arrange(col_a, col_b, col_c, col_x)
```

We will call the above solution "<code>dplyr_b</code>".  A new comparison including "<code>dplyr_b</code>" is given below.

```{r present3, fig.retina=2, fig.width=12, fig.height=8}
ggplot(data = timings[timings$method %in% qc(dplyr, base_r, dplyr_b,
                                             data.table),], 
       aes(x = rows, y = seconds)) +
  geom_point(aes(color = method)) + 
  geom_smooth(aes(color = method),
              se = FALSE) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_manual(values = color_map[qc(dplyr, base_r, dplyr_b,
                                             data.table)]) +
  ggtitle("grouped ranked selection task time by rows and method",
          subtitle = "log-log trend shown; comparing dplyr, base-R, and data.table") 
```

Notice in the above graph we have also added <code>data.table</code> results (and left out the earlier <code>Pandas</code> results).  At no scale tested does either of the <code>dplyr</code> solutions match the performance of either of base-<code>R</code> or <code>data.table</code>. The ratio of the runtime of the first (or more natual) <code>dplyr</code> solution over the <code>data.table</code> runtime (<code>data.table</code> being by far the best solution) is routinely over 80 to 1.



```{r present5, fig.retina=2, fig.width=12, fig.height=8}
means <- timings %.>%
  project_nse(., 
              groupby = c("method", "rows"), 
              seconds = mean(seconds)) %.>%
  pivot_to_rowrecs(., 
                   columnToTakeKeysFrom = "method",
                   columnToTakeValuesFrom = "seconds",
                   rowKeyColumns = "rows") %.>%
  extend_nse(., 
             ratio_a = dplyr/data.table,
             ratio_b = dplyr_b/data.table) %.>%
  orderby(., "rows") %.>%
  as.data.frame(.)

m2 <- means %.>%
  select_columns(., 
                 qc(rows, ratio_a, ratio_b)) %.>%
  unpivot_to_blocks(.,
                    nameForNewKeyColumn = "comparison",
                    nameForNewValueColumn = "ratio",
                    columnsToTakeFrom = qc(ratio_a, ratio_b))
  
ggplot(data = m2, aes(x = rows, y = ratio, color = comparison)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  scale_x_log10() + 
  scale_y_log10(
    breaks = 2^{0:8},
    minor_breaks = 1:128) + 
  scale_color_manual(values = as.character(color_map[qc(dplyr, dplyr_b)])) +
  geom_hline(yintercept = 1, color = "darkgray") + 
  ggtitle("ratio of dplyr runtime to data.table runtime",
          subtitle = "grouped rank selection task")
```



We also tested an [<code>rqdatatable</code>](https://CRAN.R-project.org/package=rqdatatable) solution.  <code>rqdatatable</code> uses <code>data.table</code> to implement the [<code>rquery</code>](https://CRAN.R-project.org/package=rqdatatable) data manipulation grammar, so it has more overhead than <code>data.table</code>.

Full results are below (and all code and results are [here](https://github.com/WinVector/rqdatatable/blob/master/extras/GroupedRankFilter2.md)).

```{r present4, fig.retina=2, fig.width=12, fig.height=8}
ggplot(data = timings, aes(x = rows, y = seconds)) +
  geom_line(data = smooths,
            alpha = 0.7,
            linetype = 2,
            aes(group = method, color = method)) +
  geom_point(data = timings, aes(color = method)) + 
  geom_smooth(data = timings, aes(color = method),
              se = FALSE) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_manual(values = color_map) +
  ggtitle("grouped ranked selection task time by rows and method",
          subtitle = "log-log trend shown; showing all results") +
  facet_wrap(~method_family, ncol=1, labeller = "label_both")

knitr::kable(means[, 
                   qc(rows, base_r, data.table, 
                      dplyr, dplyr_b, 
                      pandas_reticulate, rqdatatable)])

knitr::kable(means[, 
                   qc(rows, data.table, 
                      dplyr, dplyr_b, 
                      ratio_a, ratio_b)])
```
